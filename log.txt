Bhavya:

      Time     Time
Date  Started  Spent Work completed
----  -------  ----  --------------
11/08   4:00pm  1:00 met with Duncan, came up with project ideas
11/15   2:00pm  1:00 met with Duncan, finalised 2048
12/10   4:00pm  2:00 met with Duncan, finalised approach & plan, started code
12/11   12:00pm 3:00 wrote random and greedy agents, tested code
12/13   3:00pm  5:00 wrote and fine-tuned MCTS agent
12/15   12:00pm 2:00 changed MCTS to average over random states, tested MCTS agent
12/17   10:00am 6:00 evaluated agents, wrote up
               ----
               20:00  TOTAL time spent

Duncan:

      Time     Time
Date  Started  Spent Work completed
----  -------  ----  --------------
11/08   4:00pm  1:00 met with Bhavya, came up with project ideas
11/15   2:00pm  1:00 met with Bhavya, finalised 2048
12/10   4:00pm  2:00 met with Bhavya, finalised approach & plan, started code
12/10   8:00pm  2:00 implemented 2048 game logic, wrote code for running and testing agents
12/12   2:00pm  3:00 implemented and tested expectimax agent
12/17   10:00pm 3:00 implemented approximate q-learning agent
12/18   4:00pm  3:00 fine-tuned q-learning agent and completed documentation
               ----
               15:00  TOTAL time spent

Discussions

Overall, the MCTS agent performs the best, followed by the expectimax agent (depth 3).
We found it surprising that the Linear Approximate Q-Learning (LAQN) agent was not as performant as these two.

A close look at the structure of the 2048 game can help explain this. 2048 has a huge state space,
in the order of 10^17. In addition, the random factor is very high. A random node can have a branching
factor of at most 28.  All of this means that a model-free, pretrained agent like an LAQN agent
has a difficult time learning enough states, especially state close to the end game, to end up with
a good policy. Whereas expectimax and MCTS make decisions on the fly, performing searches for each step,
which allows them to outperform the LAQN agent.

Finally, there are multiple different paths to the same state in a tree. This means that expectimax will
spend a lot of time searching through repeated nodes. However, MCTS can avoid this by focusing its
search on high-payoff actions.

